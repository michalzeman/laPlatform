akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  #log-config-on-start = on

  # Logging of Dead Letters
  log-dead-letters = 10
  log-dead-letters-during-shutdown = on

  actor {
    //provider = "akka.cluster.ClusterActorRefProvider"
    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 4
    }

    debug {
      # enable DEBUG logging of all AutoReceiveMessages (Kill, PoisonPill et.c.)
      autoreceive = on
    }

//    deployment {
//      /ingestKafkaProducerSupervisorActor/kafkaIngestProducer {
//        router = round-robin-pool
//        nr-of-instances = 2
//      }
//    }
  }

  remote {
    # The port clients should connect to. Default is 2552.
    netty.tcp.port = 4712

  }

  # Properties for akka.kafka.ConsumerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  kafka.consumer {
    # Tuning property of scheduled polls.
    poll-interval = 50ms

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that blocking of the thread that
    # is executing the stage will be blocked.
    poll-timeout = 50ms

    # The stage will be await outstanding offset commit requests before
    # shutting down, but if that takes longer than this timeout it will
    # stop forcefully.
    stop-timeout = 30s

    # How long to wait for `KafkaConsumer.close`
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `TimeoutException`.
    commit-timeout = 15s

    # If the KafkaConsumer can't connect to the broker the poll will be
    # aborted after this timeout. The KafkaConsumerActor will throw
    # org.apache.kafka.common.errors.WakeupException which will be ignored
    # until max-wakeups limit gets exceeded.
    wakeup-timeout = 3s

    # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
    max-wakeups = 10

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = false
    }
  }
}

http {
  interface="0.0.0.0"
  port = 8080
  port = ${?PORT}
}

kafka.producer {
  bootstrap.servers="localhost:9092"
  bootstrap.servers=${?VALUE}
  client.id="KafkaPredictProducer"
  client.id=${?VALUE}
  key.serializer="org.apache.kafka.common.serialization.IntegerSerializer"
  key.serializer=${?VALUE}
  value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  value.serializer=${?VALUE}
  topic="prediction"
  topic=${?VALUE}
}
